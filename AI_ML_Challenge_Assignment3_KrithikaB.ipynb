{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install sentencepiece\n",
        "\n",
        "# Pre-trained model for English to Hindi translation-MarianMTModel\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\") #from english to hindi\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9Ba50d-51pF",
        "outputId": "d8878d71-b1b4-44e9-acb5-c5b1063e5a26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny3jybTt55-D",
        "outputId": "0d9ab670-1007-4ae2-d8ca-2aa41524d528"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def spot_nouns_verbs_custom(en_sentence):\n",
        "    # Tokenize the input English sentence into words\n",
        "    words = word_tokenize(en_sentence)\n",
        "\n",
        "    # part-of-speech tagging on the words\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    # Auxiliary verbs in English\n",
        "    auxiliary_verbs = ['am', 'is', 'are', 'was', 'were', 'has', 'had']\n",
        "\n",
        "    # Nouns and verbs from the tagged words\n",
        "    nouns = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "    verbs = [word for word, pos in tagged_words if pos.startswith('VB') and word not in auxiliary_verbs]\n",
        "\n",
        "    # Lemmatize verbs to bring them to their base form\n",
        "    verbs = [lemmatizer.lemmatize(verb, pos='v') for verb in verbs]\n",
        "\n",
        "    # Dictionary to store English and Hinglish translations\n",
        "    translated_words = {\n",
        "        'feedback': 'feedback',\n",
        "        'definitely': 'निश्चितरूप ',\n",
        "        'section': 'खण्ड'\n",
        "    }\n",
        "\n",
        "    # Translation of nouns and verbs and add them to the dictionary\n",
        "    for noun in nouns:\n",
        "        hin_noun = hin_translation(noun)\n",
        "        translated_words[noun] = hin_noun\n",
        "\n",
        "    for verb in verbs:\n",
        "        hin_verb = hin_translation(verb)\n",
        "        # Take the first part of the Hinglish translation (removes extra details)\n",
        "        modified_value = hin_verb.split(' ', 1)[0]\n",
        "        translated_words[verb] = modified_value\n",
        "\n",
        "    return translated_words"
      ],
      "metadata": {
        "id": "CLoQdxk86K9a"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def hin_translation(en_sentence):\n",
        "    # Encode the English sentence using the Hinglish model\n",
        "    inputs = tokenizer.encode(en_sentence, return_tensors=\"pt\")\n",
        "    translated_id = model.generate(inputs, max_length=150, num_return_sequences=1, num_beams=4)\n",
        "    # Decode the generated Hinglish text and handle ZWJ characters\n",
        "    translated_output = tokenizer.decode(translated_id[0], skip_special_tokens=True)\n",
        "    translated_output = translated_output.replace('\\u200d', '')  # Handling ZWJ characters\n",
        "    return translated_output"
      ],
      "metadata": {
        "id": "Gwhit4bo6NAT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_hinglish_custom(en_sentence):\n",
        "    # Get nouns and verbs translations\n",
        "    nouns = spot_nouns_verbs_custom(en_sentence)\n",
        "\n",
        "    # Translate the entire English sentence to Hinglish\n",
        "    hin_text = hin_translation(en_sentence)\n",
        "\n",
        "    # Replace translated nouns and verbs in the Hinglish text\n",
        "    for eng_word, hin_word in nouns.items():\n",
        "        hin_text = hin_text.replace(hin_word, eng_word)\n",
        "\n",
        "    return hin_text"
      ],
      "metadata": {
        "id": "5NDTRsBj6Upo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Switching Hindi nouns to English nouns to keep certain words in English\n",
        "def noun_switch(nouns, hinglish_text):\n",
        "    for key, value in nouns.items():\n",
        "        matches = re.findall(r'\\b' + re.escape(value) + r'\\b', hinglish_text)\n",
        "        for match in matches:\n",
        "            hinglish_text = hinglish_text.replace(match, key)\n",
        "    return hinglish_text"
      ],
      "metadata": {
        "id": "_KVVXd7V6Yb2"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle plural forms in Hinglish\n",
        "def handle_plural_hinglish(hin_text):\n",
        "    # Define a dictionary of plural forms to be replaced\n",
        "    plural_replacements = {\n",
        "        \"productsों का\": \"products का\"\n",
        "    }\n",
        "\n",
        "    # Replace plural forms in Hinglish based on the dictionary\n",
        "    for plural, singular in plural_replacements.items():\n",
        "        hin_text = hin_text.replace(plural, singular)\n",
        "\n",
        "    return hin_text"
      ],
      "metadata": {
        "id": "4EiAU1ACFVnv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentences\n",
        "en_sentences = [\n",
        "    \"1. Definitely share your feedback in the comment section.\",\n",
        "    \"2. So even if it's a big video, I will clearly mention all the products.\",\n",
        "    \"3. I was waiting for my bag.\",\n",
        "]\n",
        "\n",
        "for en_sentence in en_sentences:\n",
        "    # Translate each English sentence to Hinglish and print the results\n",
        "    hinglish_translation = translate_to_hinglish_custom(en_sentence)\n",
        "    # Handle plural forms in Hinglish\n",
        "    hinglish_translation = handle_plural_hinglish(hinglish_translation)\n",
        "    #hinglish_translation = hinglish_translation.replace(\"productsों का\", \"products का\") #handle plural\n",
        "    print(f\"English: {en_sentence}\")\n",
        "    print(f\"Hinglish: {hinglish_translation}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpQecA8B6c1W",
        "outputId": "0d9a7208-8d43-4160-89a4-8ede5c347621"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: 1. Definitely share your feedback in the comment section.\n",
            "Hinglish: 1 निश्चित रूप से comment खण्ड में आपकी feedback share करें.\n",
            "\n",
            "English: 2. So even if it's a big video, I will clearly mention all the products.\n",
            "Hinglish: 2 अगर यह एक बड़ा video है, तो भी मैं स्पष्ट रूप से सभी products का mention करेंगे।\n",
            "\n",
            "English: 3. I was waiting for my bag.\n",
            "Hinglish: 3 मैं अपने बैग के लिए wait कर रहा था.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentences-new list(apart from given)\n",
        "en_sentences = [\n",
        "    \"1. She goes to school daily with her father.\",\n",
        "    \"2. I watch television daily.\",\n",
        "    \"3. He likes to drink coffee.\",\n",
        "]\n",
        "\n",
        "for en_sentence in en_sentences:\n",
        "    # Translate each English sentence to Hinglish and print the results\n",
        "    hinglish_translation = translate_to_hinglish_custom(en_sentence)\n",
        "    # Handle plural forms in Hinglish\n",
        "    hinglish_translation = handle_plural_hinglish(hinglish_translation)\n",
        "    #hinglish_translation = hinglish_translation.replace(\"productsों का\", \"products का\") #handle plural\n",
        "    print(f\"English: {en_sentence}\")\n",
        "    print(f\"Hinglish: {hinglish_translation}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7V_XwsPJAOe",
        "outputId": "b5315365-6b68-47e9-b2ce-e2f7e4087c5a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: 1. She goes to school daily with her father.\n",
            "Hinglish: 1. वह अपने father के साथ हर दिन school जाती है.\n",
            "\n",
            "English: 2. I watch television daily.\n",
            "Hinglish: 2. मैं हर दिन television देखता हूँ.\n",
            "\n",
            "English: 3. He likes to drink coffee.\n",
            "Hinglish: 3 वह coffee पीना पसंद करता है.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}